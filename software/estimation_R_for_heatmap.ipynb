{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily R value derived from number of cases reported in LA county\n",
    "Mehrdad Kiamari, Bhaskar Krishnamachari - June 2020\n",
    "\n",
    "To monitor the severity of any epidemic, it is crucial to look at $R_t$ which is a value representing the effective reproduction number (the number of individuals who are infected per infectious individual at time $t$) of the disease. \n",
    "\n",
    "Regarding $R_t$, the epidemic will exponentially grow among the population when $R_t >> 1$. However, the epidemic sloowly disappear as $R_t<1$. Since restirctions would eventually impactts $R_t$, this measure can guide authorities to take appropriate actions regarding tightening or loosing restrictions for the sake of having economic prosperity and human safety.   \n",
    "\n",
    "In this code, we aim at estimating daily R value of COVID-19 in LA county. Our approach is universal and can be utilized for any area. We use SIR model, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dS}{dt} &= -\\beta \\frac{SI}{N}\\\\\n",
    "\\frac{dI}{dt} &= +\\beta \\frac{SI}{N} - \\sigma I\\\\\n",
    "\\frac{dR}{dt} &= \\sigma I\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $S$, $I$, and $R$ represent the number of Susceptible, Infected, and Recovered people in a population size of $N$. Regarding the parameter $\\sigma = \\frac{1}{D_i}$, $D_i$ represents the average infectious days.\n",
    "\n",
    "As far as $R$ is concerned, it is equal to $\\frac{\\beta}{\\sigma}$. Our idea is to estimate $\\beta$ at each time from the above differential equation which involves $\\frac{dI}{dt}$, then calculate the corresponding $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import numpy as np\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from datetime import date \n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "#from gekko import GEKKO\n",
    "from matplotlib.dates import date2num, num2date\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_comm = 'sangabriel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary consisting of communities and their population as keys and values, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('population.json') as json_file_pop:\n",
    "\tdata_population = json.load(json_file_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check if the communitiy exists in the dictionary for population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_community_is_in_dic_pop(community_name):\n",
    "\twith open('population.json') as json_file_pop:\n",
    "\t\tdata_population = json.load(json_file_pop)\n",
    "\t\ttemp = [val for key,val in data_population.items() if community_name == key.strip().split('--')[0]]\n",
    "\t\tif len(temp)==1:\n",
    "\t\t\t#print(community_name,\"found\")\n",
    "\t\t\treturn True\n",
    "\t\t#print(community_name,\"NOT found\")    \n",
    "\t\treturn False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Class and Functions\n",
    "\n",
    "Each community is an object with few attributes such as name, number of daily or cumulative cases, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_beta_for_single_time_polynomial(next_I,curr_I,sigma,N,prev_beta,k):\n",
    "#      \n",
    "\tif curr_I != 0:\n",
    "# \t\tm = GEKKO()             # create GEKKO model\n",
    "# \t\tbeta = m.Var(value=1.0)      # define new variable, initial value=0\n",
    "# \t\tm.Equations([(beta-sigma)*curr_I -  (beta/N)*(curr_I**2) == next_I - curr_I ]) # equations\n",
    "# \t\tm.solve(disp=False)     # solve\n",
    "# \t\t# not being negative\n",
    "# \t\toutput = max(beta.value[0],0)\n",
    "\t\toutput = (next_I - curr_I+sigma*curr_I)/(curr_I-(k/N)*curr_I**2)\n",
    "\telse:\n",
    "\t\toutput = prev_beta\n",
    "\n",
    "\t#print(beta.value[0])\n",
    "\treturn output \t\n",
    "def solve_beta_for_single_time_exponential(next_I,curr_I,sigma,N,prev_beta,k,cap_for_searching_exact_sol):\n",
    "\n",
    "\t#clear_output(wait=True)    \n",
    "\t#print(\"curr\", curr_I, \"next\", next_I)\n",
    "# \tif next_I != 0 and curr_I != 0 and next_I != curr_I:\n",
    "# \t\tm = GEKKO()             # create GEKKO model\n",
    "# \t\tbeta = m.Var(value=1.0)      # define new variable, initial value=0\n",
    "# \t\tm.Equations([((1/(beta-sigma))*m.log(next_I/((beta-sigma)-beta*next_I/N))) -  ((1/(beta-sigma))*m.log(curr_I/((beta-sigma)-beta*curr_I/N))) == 1.0]) # equations\n",
    "# \t\tm.solve(disp=False)     # solve\n",
    "# \t\toutput = beta.value[0]\n",
    "# \telse:\n",
    "# \t\toutput = solve_beta_for_single_time_polynomial(next_I,curr_I,sigma,N,prev_beta)\n",
    "##################################\n",
    "# \tdata = (next_I,curr_I,sigma,N)\n",
    "# \tbeta_guess = .2\n",
    "#\toutput = fsolve(function_for_solver, beta_guess, args=data)\n",
    "#################################\n",
    "\t#cap_for_searching_exact_sol = 10\n",
    "\toutput = max(0,solve_beta_for_single_time_polynomial(next_I,curr_I,sigma,N,prev_beta,k))\n",
    "# \tif output > cap_for_searching_exact_sol and next_I != 0 and curr_I != 0 and next_I != curr_I:\n",
    "# \t\tm = GEKKO()             # create GEKKO model\n",
    "# \t\tbeta = m.Var(value=1.0)      # define new variable, initial value=0\n",
    "# \t\tm.Equations([((1/(beta-sigma))*m.log(k*next_I/((beta-sigma)-beta*k*next_I/N))) -  ((1/(beta-sigma))*m.log(k*curr_I/((beta-sigma)-beta*k*curr_I/N))) == 1.0]) # equations\n",
    "# \t\tm.solve(disp=False)     # solve\n",
    "# \t\toutput = beta.value[0]  \n",
    "\n",
    "\treturn output\n",
    "\n",
    "def calculating_beta_for_a_comm(name,matrix_I,population,sigma,k,cap_for_searching_exact_sol):\n",
    "\t#print(\"confirmed cases: \",matrix_I.shape[1])\n",
    "# \tprint(matrix_I.shape[1])\n",
    "\tc = matrix_I.shape[1]\n",
    "\tmatrix_beta = np.zeros((c-1,))\n",
    "\tR = np.zeros((c-1,))\n",
    "\t#for city in range(r):\n",
    "\t#prev_beta = 0\n",
    "\tfor time in range(c-1):\n",
    "\t\t#clear_output(wait=True)  \n",
    "\t\t#print(\"beta for city:\",city)\n",
    "\t\tmatrix_beta[time] = solve_beta_for_single_time_exponential(matrix_I[0,time+1],matrix_I[0,time],sigma,population,0,k,cap_for_searching_exact_sol) \n",
    "\t\t#prev_beta = matrix_beta[city,time]\n",
    "\t\tR[time] = matrix_beta[time] / sigma\n",
    "\t\t#if name == target_comm:\n",
    "\t\t#print(R[time], matrix_I[0,time+1],matrix_I[0,time])\n",
    "\treturn R\n",
    "\n",
    "#def moving_ave(matrix_I,n):\n",
    "def movingaverage (values, window):\n",
    "    weights = np.repeat(1.0, window)/window\n",
    "    sma = np.convolve(values, weights, 'valid')\n",
    "    return sma\n",
    "def mov_ave(row_matrix,window,std_gaussian):\n",
    "    data_allc = pd.DataFrame(row_matrix.transpose())\n",
    "    #print(len(data_allc),'\\n',data_allc)\n",
    "    #data_allc=data_allc.rolling(window,win_type='gaussian',min_periods=1,center=True).mean(std=std_gaussian)\n",
    "    data_allc=data_allc.rolling(window,win_type='parzen', min_periods=1,center=False).mean()\n",
    "    val_pd = (data_allc.values).transpose()       \n",
    "    #print(val_pd.shape,'alll \\n',val_pd)\n",
    "    return val_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class community:\n",
    "\tdef __init__(self,name,actual_name,Today_date):\n",
    "\t\tself.name = name\n",
    "\t\tself.Today_date = Today_date\n",
    "\t\tself.actual_name = actual_name # for displaying part of figures\n",
    "\t\t# cumulative total\n",
    "\t\tself.confirmed = np.zeros(len(range(16,self.Today_date)),dtype=int)\n",
    "\t\tself.confirmed_daily = np.zeros(len(range(16,self.Today_date)),dtype=int)\n",
    "\t\tself.infection_rate = np.zeros(len(range(16,self.Today_date-1)))\n",
    "\t\tself.risk = np.zeros(len(range(16,self.Today_date-1)))\n",
    "\t\tself.pop = 0\n",
    "\n",
    "\tdef set_population(self,val):        \n",
    "\t\tself.pop = val\n",
    "        \n",
    "\tdef set_infection_rate(self,sigma,k,cap_for_searching_exact_sol):        \n",
    "\t\tself.infection_rate = calculating_beta_for_a_comm(self.name,self.confirmed_daily,self.pop,sigma,k,cap_for_searching_exact_sol)  \n",
    "\n",
    "        \n",
    "\tdef set_risk(self,day,val):\n",
    "\t\tself.risk[day] = val#(10000)*self.infection_rate[day]*self.confirmed_daily[day]/(self.pop-self.confirmed_daily[day])\n",
    "\n",
    "\n",
    "\n",
    "# \tdef smoothing(self,n=7):        \n",
    "# \t\t#n=7      \n",
    "# \t\tpadded_I = np.zeros((1,len(self.confirmed_daily) + n-1 ))\n",
    "\n",
    "# \t\tpadded_I[0,(n-1)//2:-(n-1)//2]=self.confirmed_daily\n",
    "# \t\t#for city in range(ref_matrix_I.shape[0]):\n",
    "# \t\tself.confirmed_daily = movingaverage(padded_I[0,:],n)         \n",
    "\n",
    "\tdef smoothing(self,n=7):        \n",
    "\t\tself.confirmed_daily = mov_ave(self.confirmed_daily,n,100) \n",
    "        \n",
    "\t# for adding new entry for each community on every day \t\n",
    "\tdef check_validity_new_entry(self,day):\n",
    "\t\tindex = day - 16\n",
    "\t\tif index == 0:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\tif self.confirmed[index] >= self.confirmed[index-1]:\n",
    "\t\t\t\treturn True\n",
    "\t\t\treturn False\t\t\n",
    "\tdef update_confirmed_cases(self,day):\n",
    "\t\t#print(\"before\")\n",
    "\t\t#print(self.confirmed)\n",
    "\t\tindex = day - 16\n",
    "\t\twhile index != 0:\n",
    "\t\t\tif self.confirmed[index] < self.confirmed[index-1]:\n",
    "\t\t\t\tself.confirmed[index-1] = self.confirmed[index]\n",
    "\t\t\tindex -= 1\n",
    "\t\t#print(\"after\")\n",
    "\t\t#print(self.confirmed)\n",
    "\tdef addnumber(self,day, number):\n",
    "\t\tindex = day - 16\n",
    "\t\tself.confirmed[index] = number\n",
    "\t\tstatus_validity_of_entry = self.check_validity_new_entry(day)\n",
    "\t\tif not status_validity_of_entry:\n",
    "\t\t\tself.update_confirmed_cases(day)\n",
    "\n",
    "\t# return the confirmed cases (either daily or cumulative) for each community\t\t\n",
    "\tdef plot_info(self,type_plot):\n",
    "\t\toutput = np.zeros(len(range(16,self.Today_date)),dtype=int)\n",
    "\t\tfor index,i in enumerate(list(range(16,self.Today_date))):\n",
    "\t\t\t# for daily\n",
    "\t\t\tif type_plot == 'daily':\n",
    "\t\t\t\t# if i in self.dic_confirmed.keys():\n",
    "\t\t\t\t# \toutput[index] =  self.dic_confirmed[i]\n",
    "\t\t\t\t# else:\n",
    "\t\t\t\t# \toutput[index] = 0\n",
    "\t\t\t\toutput = self.confirmed_daily\n",
    "\t\t\t# for cumulative\n",
    "\t\t\telse:\n",
    "\t\t\t\toutput = self.confirmed\n",
    "\t\treturn output\t\n",
    "\n",
    "# get population for top selected communities    \n",
    "def get_population_vec(list_communities):\n",
    "\twith open('population.json') as json_file_pop:\n",
    "\t\tdata = json.load(json_file_pop)\n",
    "\n",
    "\t\toutput_list = []\n",
    "\t\tfor communiuty_obj in list_communities:\n",
    "\t\t\t#print(\"comm name\",communiuty_obj.name)\n",
    "\t\t\ttemp = [val for key,val in data.items() if communiuty_obj.actual_name == key.strip().split('--')[0]]\n",
    "\t\t\tif temp :\n",
    "\t\t \t\toutput_list.append(int(temp.pop().strip()))\n",
    "\t\tif len(output_list) == len(list_communities):\n",
    "\t\t\toutput = np.asarray(output_list)\n",
    "\t\telse:\n",
    "\t\t\traise NameError('The name of one of communities has NOT been found!')\t\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "def create_matrix_with_dict_as_input(dict_selected_communities,type_plot,til_date):\n",
    "\t# matrix_I  has c rows (communities) and T columns (days) \n",
    "\t#matrix_I =  np.zeros((len(list_selected_communities),len(range(16,list_selected_communities[0].Today_date)) ))\n",
    "\tmatrix_I =  np.zeros((len(dict_selected_communities),til_date ))\n",
    "\ti =0\n",
    "\tfor communiuty_obj in  dict_selected_communities.keys():\n",
    "\t\tI_s_this_community_obj = communiuty_obj.plot_info(type_plot)[:til_date]\n",
    "\t\t#print(\"row\", i, communiuty_obj.name)\n",
    "\t\tfor j,infected_at_this_day in enumerate(I_s_this_community_obj):\n",
    "\t\t\tmatrix_I[i,j] = infected_at_this_day\n",
    "\t\t\tdict_selected_communities[communiuty_obj].append(i)            \n",
    "\t\ti += 1\n",
    "\treturn dict_selected_communities, matrix_I\n",
    "    \n",
    "    \n",
    "# create matrix for number of infections for top selected communities    \n",
    "def create_matrix(list_selected_communities,type_plot,til_date):\n",
    "\t# matrix_I  has c rows (communities) and T columns (days) \n",
    "\t#matrix_I =  np.zeros((len(list_selected_communities),len(range(16,list_selected_communities[0].Today_date)) ))\n",
    "\tmatrix_I =  np.zeros((len(list_selected_communities),til_date ))\n",
    "\tfor i,communiuty_obj in  enumerate(list_selected_communities):\n",
    "\t\tI_s_this_community_obj = communiuty_obj.plot_info(type_plot)[:til_date]\n",
    "\t\t#print(\"row\", i, communiuty_obj.name)\n",
    "\t\tfor j,infected_at_this_day in enumerate(I_s_this_community_obj):\n",
    "\t\t\t matrix_I[i,j] = infected_at_this_day\n",
    "\treturn matrix_I\t \n",
    "\n",
    "# matrix I is supposed to be increasing for each community, so we fix any drop by this function\n",
    "def fix_matrix_I(matrix_I):\n",
    "    output = np.zeros_like(matrix_I)\n",
    "    output[:,0] = matrix_I[:,0]\n",
    "    r,c = matrix_I.shape[0], matrix_I.shape[1]\n",
    "    for ind_r in range(r):\n",
    "        for ind_c in range(1,c):\n",
    "            if matrix_I[ind_r,ind_c] < matrix_I[ind_r,ind_c-1]:\n",
    "                output[ind_r,ind_c] = matrix_I[ind_r,ind_c-1]\n",
    "            else:\n",
    "                output[ind_r,ind_c] = matrix_I[ind_r,ind_c]\n",
    "    return output            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region  Time Stamp\n",
       "Acton   2020-03-28    0\n",
       "        2020-03-29    0\n",
       "        2020-03-30    0\n",
       "        2020-03-31    0\n",
       "        2020-04-01    0\n",
       "Name: Number of cases, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states1 = pd.read_csv('Covid-19.csv', usecols=[0,1,4],\n",
    "                     index_col=['Region', 'Time Stamp'],\n",
    "                     parse_dates=['Time Stamp'],\n",
    "                     squeeze=True).sort_index()\n",
    "states = states1.groupby(['Region', 'Time Stamp']).sum()\n",
    "states.head()\n",
    "#print(states['Melrose']['2020-03-16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame \n",
    "Make DataFrame for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_R(ind_city,matrix_beta,sigma,U,D):  \n",
    "    r,c = matrix_beta.shape[0],matrix_beta.shape[1]\n",
    "    data={}\n",
    "    data['R'] = matrix_beta[ind_city,:]/sigma\n",
    "    data['Upper'] = U[ind_city,:]\n",
    "    data['Lower'] = D[ind_city,:]\n",
    "    #print(len(data['R']),len(data['Upper']),len(data['Lower']))\n",
    "    data['Time Stamp'] = pd.date_range(start='2020-03-16', periods=c)    \n",
    "    dataset = pd.DataFrame(data)\n",
    "    dataset.set_index(['Time Stamp'], inplace=True)    \n",
    "    #print(dataset)\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Func for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rt(result, ax, state_name):\n",
    "    \n",
    "    ax.set_title(f\"{state_name}\")\n",
    "    \n",
    "    # Colors\n",
    "    ABOVE = [1,0,0]\n",
    "    MIDDLE = [1,1,1]\n",
    "    BELOW = [0,0,0]\n",
    "    cmap = ListedColormap(np.r_[\n",
    "        np.linspace(BELOW,MIDDLE,25),\n",
    "        np.linspace(MIDDLE,ABOVE,25)\n",
    "    ])\n",
    "    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n",
    "    \n",
    "    index = result['R'].index.get_level_values('Time Stamp')\n",
    "    values = result['R'].values\n",
    "    \n",
    "    # Plot dots and line\n",
    "    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n",
    "    ax.scatter(index,\n",
    "               values,\n",
    "               s=40,\n",
    "               lw=.5,\n",
    "               c=cmap(color_mapped(values)),\n",
    "               edgecolors='k', zorder=2)\n",
    "    \n",
    "    # Aesthetically, extrapolate credible interval by 1 day either side\n",
    "    lowfn = interp1d(date2num(index),\n",
    "                     result['Lower'].values,\n",
    "                     bounds_error=False,\n",
    "                     fill_value='extrapolate')\n",
    "    \n",
    "    highfn = interp1d(date2num(index),\n",
    "                      result['Upper'].values,\n",
    "                      bounds_error=False,\n",
    "                      fill_value='extrapolate')\n",
    "    \n",
    "    extended = pd.date_range(start=pd.Timestamp('2020-03-16'),\n",
    "                             end=index[-1]+pd.Timedelta(days=1))\n",
    "    \n",
    "    ax.fill_between(extended,\n",
    "                    lowfn(date2num(extended)),\n",
    "                    highfn(date2num(extended)),\n",
    "                    color='k',\n",
    "                    alpha=.1,\n",
    "                    lw=0,\n",
    "                    zorder=3)\n",
    "\n",
    "    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n",
    "    \n",
    "    # Formatting\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    \n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.margins(0)\n",
    "    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n",
    "    ax.margins(0)\n",
    "    ax.set_ylim(0.0,10.0)\n",
    "    ax.set_xlim(pd.Timestamp('2020-03-16'), result.index.get_level_values('Time Stamp')[-1]+pd.Timedelta(days=1))\n",
    "    #fig.set_facecolor('w')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting for mean and var PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_D = 7.5\n",
    "var_D = 4.0\n",
    "\n",
    "min_k_value = 1.0\n",
    "max_k_value = 10.0\n",
    "mean_k = .5*(min_k_value+max_k_value)\n",
    "# with .997% CI\n",
    "var_k = ((max_k_value-mean_k)/3)**2\n",
    "\n",
    "cap_for_searching_exact_sol = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram - used for finding thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_plot_data(labels, bp):\n",
    "    rows_list = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        dict1 = {}\n",
    "        dict1['label'] = labels[i]\n",
    "        dict1['lower_whisker'] = bp['whiskers'][i*2].get_ydata()[1]\n",
    "        dict1['lower_quartile'] = bp['boxes'][i].get_ydata()[1]\n",
    "        dict1['median'] = bp['medians'][i].get_ydata()[1]\n",
    "        dict1['upper_quartile'] = bp['boxes'][i].get_ydata()[2]\n",
    "        dict1['upper_whisker'] = bp['whiskers'][(i*2)+1].get_ydata()[1]\n",
    "        rows_list.append(dict1)\n",
    "\n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_risk_score(communities):\n",
    "\tava_communities=[]\n",
    "\tfor comm in communities:\n",
    "\t\tif available_in_pop(comm):\n",
    "\t\t\tava_communities.append(comm)\n",
    "\tnum_day_to_investigate = len(ava_communities[0].confirmed_daily) - 1\n",
    "\t#risk_score_a_day = np.zeros((len(ava_communities),num_day_to_investigate))\n",
    "\tdata_risk , labels = []  , [] \n",
    "\tfor day in range(65,66):#num_day_to_investigate):\n",
    "\t\trisk_score_a_day = np.zeros(len(ava_communities))\n",
    "\t\tfor ind, comm in enumerate(ava_communities):\n",
    "\t\t\trisk_score_a_day[ind] = (comm.infection_rate[day]*comm.confirmed_daily[day]/(get_population_vec([comm])-comm.confirmed_daily[day]))\n",
    "\t\tmin_r , max_r = np.min(risk_score_a_day),np.max(risk_score_a_day)\n",
    "\t\trange_r = np.arange(min_r,max_r,(max_r-min_r)/100.0)\n",
    "        #[10, 2.5, 3, 4,5,6,7,8,9,10]\n",
    "\t\tplt.hist(risk_score_a_day, bins=range_r, density=False)\n",
    "\t\tdata_risk.append(risk_score_a_day)\n",
    "\t\tlabels.append(day)\n",
    "#\tplt.boxplot(data_risk[65:70],showfliers=False)\n",
    "\tbp = plt.boxplot(data_risk,labels)#showfliers=False) \n",
    "#\tdf = get_box_plot_data(labels, bp)\n",
    "#\thist_thresholds = df.mean()\n",
    "#\tprint(\"hist_thresholds\\n\",hist_thresholds[0],hist_thresholds[1],hist_thresholds[2],hist_thresholds[3])\n",
    "\tplt.show()\n",
    "#\treturn hist_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_level(val,thresholds):\n",
    "    if val < thresholds[1]:\n",
    "        return 0\n",
    "    elif thresholds[1]<val and val<=thresholds[2]:\n",
    "        #import pdb;pdb.set_trace()\n",
    "        return 1\n",
    "    \n",
    "    elif thresholds[2]<val and val<=thresholds[3]:\n",
    "        return 2\n",
    "    #elif thresholds[2]<val and val<=thresholds[3]:\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update CSV file (replace density with infection rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_in_pop(obj):\n",
    "    with open('population.json') as json_file_pop:\n",
    "        data_population = json.load(json_file_pop)\n",
    "        temp = [val for key,val in data_population.items() if obj.actual_name == key.strip().split('--')[0]]\n",
    "        if len(temp)==1:\n",
    "            #print(community_name,\"found\")\n",
    "            return True\n",
    "        #print(community_name,\"NOT found\")    \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ESTIMATION of risk score of all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv_file(dict_comm,thresholds):\n",
    "    #begining_date = date(2020, 3, 16)\n",
    "    data_map = pd.read_csv('Covid-19-density.csv')\n",
    "    \n",
    "    data_map['Date'] = pd.to_datetime(data_map['Time Stamp'], format='%m-%d-%Y')\n",
    "    data_map['Date-Start'] = '03-16-2020'\n",
    "    data_map['Date-Start'] = pd.to_datetime(data_map['Date-Start'], format='%m-%d-%Y')\n",
    "    data_map['Time-Index'] = (data_map['Date']-data_map['Date-Start']).dt.days\n",
    "    data_map['Risk-Score'] = -1.0\n",
    "    data_map['Risk-Level'] = -1\n",
    "    #data_map.head(20)\n",
    "    #print(data_map['date'])\n",
    "    #return 0\n",
    "    for ind in data_map.index: \n",
    "        # getting name of comm and do regular expressions\n",
    "        city = data_map['Region'][ind]\n",
    "        processed_city = city.strip().lower().replace(' ','')\n",
    "        prefixex = ['cityof','losangeles-','unincorporated-']\n",
    "        for word in prefixex:\n",
    "            name_of_community = processed_city.replace(word,'')\n",
    "        # get day\n",
    "        day = data_map['Time-Index'][ind]\n",
    "        #print(day,type(day))\n",
    "        comm_obj = dict_comm[name_of_community]\n",
    "        if day >= len(comm_obj.infection_rate):\n",
    "            data_map.drop(ind)\n",
    "        else:\n",
    "        #print(len(comm_obj.infection_rate))\n",
    "            #print(\"-->\",comm_obj.name,comm_obj.confirmed_daily.shape)\n",
    "            #curr_comm = []\n",
    "            #curr_comm.append(comm_obj)\n",
    "            ####not_found_list = ['avalon','parklabrea','baldwinpark','bassett']\n",
    "            if available_in_pop(comm_obj):\n",
    "                data_map['Risk-Score'][ind] = (10000)*comm_obj.infection_rate[day]*comm_obj.confirmed_daily[0,day]/(get_population_vec([comm_obj]))\n",
    "                data_map['Risk-Level'][ind] = decide_level(data_map['Risk-Score'][ind],thresholds)\n",
    "        #- begining_date).dt.days \n",
    "        #print(day,type(day))\n",
    "        #ind_day = row['Time Stamp'] - begining_date\n",
    "        #print(ind_day,row['Time Stamp'])\n",
    "    #print(data_map[data_map['Risk-Level']==3])\n",
    "    #print(data_map.loc[data_map['Risk-Score'] == -1.0].Region.unique())\n",
    "    data_map.to_csv('Covid-19-R.csv', columns = ['Time Stamp','Region','Latitude','Longitude','Risk-Score','Risk-Level'])\n",
    "    #print(data_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PREDICTION of risk score of all communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_R_LA(name,matrix_beta, ind_last_day_to_consider_for_reg, num_last_days_used_in_reg,poly_degree):\n",
    "    num_cities,num_days = matrix_beta.shape[0],matrix_beta.shape[1]\n",
    "    #x = np.arange(number_last_days_to_use)\n",
    "    #y_pred = np.zeros((num_day_to_pred,))\n",
    "    #for city in range(num_cities):\n",
    "    print(matrix_beta.shape)\n",
    "    y = matrix_beta[0,ind_last_day_to_consider_for_reg-num_last_days_used_in_reg:ind_last_day_to_consider_for_reg+1]\n",
    "    x = (1.0)*np.arange(ind_last_day_to_consider_for_reg-num_last_days_used_in_reg,ind_last_day_to_consider_for_reg+1)\n",
    "    print(\"shape beta\",matrix_beta.shape, ind_last_day_to_consider_for_reg-num_last_days_used_in_reg,ind_last_day_to_consider_for_reg+1)\n",
    "    print(\"len(x)\",len(x))\n",
    "    print(\"matrix_beta.shape[1]\",matrix_beta.shape[1],'\\n',len(y))\n",
    "    #coeff = np.polynomial.polynomial.polyfit(x, y, poly_degree)\n",
    "    coeff = np.polyfit(x, y,1)#, 1, w = np.arange(x))\n",
    "    model = np.poly1d(coeff)\n",
    "    #for ind_future in range(num_day_to_pred):\n",
    "    #    pred_y[ind_future] = model(number_last_days_to_use+ind_future)\n",
    "    #plt.plot(np.arange(num_days),matrix_beta[city,:],'o-r')\n",
    "    x_future = np.arange(ind_last_day_to_consider_for_reg+1,ind_last_day_to_consider_for_reg+2)\n",
    "    print(x_future,model(x_future))\n",
    "    return model(x_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv_file_PREDICTION(dict_comm,thresholds):\n",
    "    #begining_date = date(2020, 3, 16)\n",
    "    data_map = pd.read_csv('Covid-19-density.csv')\n",
    "    \n",
    "    data_map['Date'] = pd.to_datetime(data_map['Time Stamp'], format='%m-%d-%Y')\n",
    "    data_map['Date-Start'] = '03-16-2020'\n",
    "    data_map['Date-Start'] = pd.to_datetime(data_map['Date-Start'], format='%m-%d-%Y')\n",
    "    data_map['Time-Index'] = (data_map['Date']-data_map['Date-Start']).dt.days\n",
    "    data_map['Risk-Score'] = -1.0\n",
    "    data_map['Risk-Level'] = -1\n",
    "    #data_map.head(20)\n",
    "    #print(data_map['date'])\n",
    "    #return 0\n",
    "    for ind in data_map.index: \n",
    "        # getting name of comm and do regular expressions\n",
    "        city = data_map['Region'][ind]\n",
    "        processed_city = city.strip().lower().replace(' ','')\n",
    "        prefixex = ['cityof','losangeles-','unincorporated-']\n",
    "        for word in prefixex:\n",
    "            name_of_community = processed_city.replace(word,'')\n",
    "        # get day\n",
    "        day = data_map['Time-Index'][ind]\n",
    "        #print(day,type(day))\n",
    "        comm_obj = dict_comm[name_of_community]\n",
    "        if day >= len(comm_obj.infection_rate):\n",
    "            data_map.drop(ind)\n",
    "        else:\n",
    "        #print(len(comm_obj.infection_rate))\n",
    "            #print(\"-->\",comm_obj.name)\n",
    "            #curr_comm = []\n",
    "            #curr_comm.append(comm_obj)\n",
    "            ####not_found_list = ['avalon','parklabrea','baldwinpark','bassett']\n",
    "            if available_in_pop(comm_obj):\n",
    "                data_map['Risk-Score'][ind] = comm_obj.risk[day]#(10000)*comm_obj.infection_rate[day]*comm_obj.confirmed_daily[day]/(get_population_vec([comm_obj])-comm_obj.confirmed_daily[day])\n",
    "                data_map['Risk-Level'][ind] = decide_level(data_map['Risk-Score'][ind],thresholds)\n",
    "        #- begining_date).dt.days \n",
    "        #print(day,type(day))\n",
    "        #ind_day = row['Time Stamp'] - begining_date\n",
    "        #print(ind_day,row['Time Stamp'])\n",
    "    #print(data_map[data_map['Risk-Level']==3])\n",
    "    print(data_map.loc[data_map['Risk-Score'] == -1.0].Region.unique())\n",
    "    data_map.to_csv('Covid-19-R-Prediction.csv', columns = ['Time Stamp','Region','Latitude','Longitude','Risk-Score','Risk-Level'])\n",
    "    #print(data_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Risk Scores across Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_risk_score(communities):\n",
    "\tava_communities=[]\n",
    "\tfor comm in communities:\n",
    "\t\tif available_in_pop(comm):\n",
    "\t\t\tava_communities.append(comm)\n",
    "\tnum_day_to_investigate = len(ava_communities[0].confirmed_daily) - 1\n",
    "\t#risk_score_a_day = np.zeros((len(ava_communities),num_day_to_investigate))\n",
    "\tdata_risk , labels = []  , [] \n",
    "\tfor day in range(num_day_to_investigate-1,num_day_to_investigate):\n",
    "\t\trisk_score_a_day = np.zeros(len(ava_communities))\n",
    "\t\tfor ind, comm in enumerate(ava_communities):\n",
    "\t\t\t#risk_score_a_day[ind] = comm.infection_rate[day]\n",
    "\t\t\trisk_score_a_day[ind] = (10000)*(comm.infection_rate[day]*comm.confirmed_daily[0,day]/(get_population_vec([comm])))\n",
    "\t\tmin_r , max_r = np.min(risk_score_a_day),np.max(risk_score_a_day)\n",
    "\t\trange_r = np.arange(min_r,max_r+1.0,(max_r-min_r)/100.0)\n",
    "        #####[10, 2.5, 3, 4,5,6,7,8,9,10]\n",
    "\t\tplt.hist(risk_score_a_day, bins=range_r, density=False)\n",
    "\tplt.title('Histogram of Risk Score')  \n",
    "\tplt.ylabel('Frequency of Communities') \n",
    "\tplt.xlabel('Risk Score')    \n",
    "\tplt.show()\n",
    "\t#return hist_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_R_val(communities):\n",
    "\tava_communities=[]\n",
    "\tfor comm in communities:\n",
    "\t\tif available_in_pop(comm):\n",
    "\t\t\tava_communities.append(comm)\n",
    "\tnum_day_to_investigate = len(ava_communities[0].confirmed_daily) - 1\n",
    "\t#risk_score_a_day = np.zeros((len(ava_communities),num_day_to_investigate))\n",
    "\tdata_risk , labels = []  , [] \n",
    "\tfor day in range(num_day_to_investigate-1,num_day_to_investigate):\n",
    "\t\trisk_score_a_day = np.zeros(len(ava_communities))\n",
    "\t\tfor ind, comm in enumerate(ava_communities):\n",
    "\t\t\trisk_score_a_day[ind] = comm.infection_rate[day]\n",
    "\t\t\t#risk_score_a_day[ind] = (10000)*(comm.infection_rate[day]*comm.confirmed_daily[day]/(get_population_vec([comm])-comm.confirmed_daily[day]))\n",
    "\t\tmin_r , max_r = np.min(risk_score_a_day),np.max(risk_score_a_day)\n",
    "\t\trange_r = np.arange(min_r,max_r+1.0,(max_r-min_r)/100.0)\n",
    "        #####[10, 2.5, 3, 4,5,6,7,8,9,10]\n",
    "\t\tplt.hist(risk_score_a_day, bins=range_r, density=False)\n",
    "\tplt.title('Histogram of $R_t$')  \n",
    "\tplt.ylabel('Frequency of Communities') \n",
    "\tplt.xlabel('$R_t$')    \n",
    "\tplt.show()\n",
    "\t#return hist_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decide_level(val,thresholds):\n",
    "#     if val<=thresholds[0]:\n",
    "#         return 0\n",
    "#     elif thresholds[0]<val and val<=thresholds[1]:\n",
    "#         return 1\n",
    "#     elif thresholds[1]<val and val<=thresholds[2]:\n",
    "#         return 2\n",
    "#     #elif thresholds[2]<val and val<=thresholds[3]:\n",
    "#     else:\n",
    "#         return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrdad/Downloads/lacounty_covid_data/mypython/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/mehrdad/Downloads/lacounty_covid_data/mypython/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def main(num_day_for_moving_ave,top_i_comm, type_plot,Today_date,future_day_to_be_predicted,til_date,criteria, sigma,gamma):\n",
    "\tdict_county = {} # dictionary of all community objects\n",
    "\tlist_communities = [] # list of all community objects\n",
    "\tlist_pair = []\t\t\t\n",
    "\twith open('lacounty_covid.json') as json_file:\n",
    "\t\tdata = json.load(json_file)\n",
    "\t\t# record all data by creating community classes and fill out their variables \n",
    "\t\tfor day in sorted([int(k) for k in data.keys()]):\n",
    "\t\t\tif day < Today_date :\n",
    "\t\t\t\t#print(day, Today_date )\n",
    "\t\t\t\tfor i in range(len(data[str(day)])):\n",
    "\t\t\t\t\tactual_name_of_community = \tdata[str(day)][i][0].strip()\n",
    "\t\t\t\t\tname_of_community = data[str(day)][i][0].strip().lower().replace(' ','')\n",
    "\t\t\t\t\t# cleaning city names, removing following prefixes\n",
    "\t\t\t\t\tprefixex = ['cityof','losangeles-','unincorporated-']\n",
    "\t\t\t\t\tfor word in prefixex:\n",
    "\t\t\t\t\t\tname_of_community = name_of_community.replace(word,'') \n",
    "\t\t\t\t\t# cleaning confirmed number, e.g. <1 will be 1\n",
    "\t\t\t\t\tconfirmed_cases   = data[str(day)][i][0].strip().lower(),re.sub(\"[^0-9]\", \"\", data[str(day)][i][1].strip())\n",
    "\t\t\t\t\tif name_of_community not in dict_county.keys():\n",
    "\t\t\t\t\t\tdict_county[name_of_community] = community(name_of_community,actual_name_of_community,Today_date)\n",
    "\t\t\t\t\t\tlist_communities.append(dict_county[name_of_community ])  \n",
    "\t\t\t\t\t\tdict_county[name_of_community].addnumber(day,int(confirmed_cases[1]))\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tdict_county[name_of_community].addnumber(day,int(confirmed_cases[1]))\n",
    "\n",
    "\n",
    "               \n",
    "\t\tdict_comm_for_update_csv = {}                        \n",
    "\t\t# get daily cases of all communities because the cumulative is already obtained\n",
    "\t\tfor communiuty_obj in list_communities:\n",
    "\t\t\tdict_comm_for_update_csv[communiuty_obj.name] = communiuty_obj\n",
    "\t\t\tfor index in range(len(communiuty_obj.confirmed)):\n",
    "\t\t\t\tif index == 0:\n",
    "\t\t\t\t\tcommuniuty_obj.confirmed_daily[index] = communiuty_obj.confirmed[index]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcommuniuty_obj.confirmed_daily[index] = communiuty_obj.confirmed[index] - communiuty_obj.confirmed[index-1]\t\n",
    "\n",
    "###==============================================================================        \n",
    "# # active for community-wise        \n",
    "# \t\tlist_selected_communities=[]\n",
    "# \t\tfor comm in list_communities:\n",
    "# \t\t\tfor word in ['westhollywood','eastlosangeles','sanpedro','castaic']:\n",
    "# \t\t\t\tif comm.name in word:\n",
    "# \t\t\t\t\tlist_selected_communities.append(comm)\n",
    "# # \t\tprint(len(list_selected_communities))\n",
    "# # \t\tfor comm in list_selected_communities:\n",
    "# # \t\t\tprint(comm.actual_name)\n",
    "# # \t\treturn  0        \n",
    "###==============================================================================\n",
    "\n",
    "\n",
    "\t\ttop_i_comm = len(list_communities)\n",
    "\t\tdict_key_comm_obj_val_inf_rate = {}        \n",
    "\t\t# find communiuty with highest \n",
    "\t\tlist_selected_communities = []\n",
    "\t\tdays = list(range(1,Today_date-16+1))\n",
    "\t\tnewlist = sorted(list_communities,key=lambda x: x.confirmed[-1], reverse=True)\n",
    "\t\tfor en,communiuty_obj in enumerate(newlist):\n",
    "\t\t\tif communiuty_obj.name != '-investigatedcases' and communiuty_obj.name !='-underinvestigation'and top_i_comm > 0:\n",
    "\t\t\t\t# append this city to the list\n",
    "\t\t\t\tif check_if_community_is_in_dic_pop(communiuty_obj.actual_name):# and communiuty_obj.name=='castatic':\n",
    "\t\t\t\t\tlist_selected_communities.append(communiuty_obj)\n",
    "\t\t\t\t\tdict_key_comm_obj_val_inf_rate[communiuty_obj] = []\n",
    "\t\t\t\t\t#plt.plot(days, communiuty_obj.plot_info(type_plot),'o-',label = communiuty_obj.actual_name)\n",
    "\t\t\t\t\ttop_i_comm -= 1\n",
    "\t\t#print(len(list_selected_communities))\n",
    "\n",
    "\t\t#create_csv_file(list_selected_communities)\n",
    "\t\t# create matrix of I for top communities (highest number of confirmed cases)\n",
    "\t\t# matrix_I is matrix I for top communities until \"til_date\" (for training)\n",
    "\t\t# matrix_I = create_matrix(list_selected_communities, type_plot,til_date)\n",
    "\t\t# ref_matrix_I is matrix I for top communities for all days \t\t\t\t\n",
    "\n",
    "\t\t############################################################\n",
    "\t\t############### Set population for each comm ###############\n",
    "\t\tfor communiuty_obj in list_selected_communities:\n",
    "\t\t\tcommuniuty_obj.set_population(get_population_vec([communiuty_obj])[0])\n",
    "\t\t\t#print(communiuty_obj.name,communiuty_obj.pop)\n",
    "\n",
    "            \n",
    "\t\t############################################################\n",
    "\t\t############### smoothing infected cases for each comm ######   \n",
    "\t\tfor communiuty_obj in list_selected_communities:\n",
    "\t\t\t#print(communiuty_obj.name,communiuty_obj.confirmed_daily)\n",
    "\t\t\tcommuniuty_obj.smoothing(num_day_for_moving_ave)\n",
    "\t\t\t#print(communiuty_obj.name,communiuty_obj.confirmed_daily)\n",
    "   \n",
    "\n",
    "\n",
    "            \n",
    "\t\t############################################################\n",
    "\t\t############### calculating R for each comm ###### \n",
    "\t\tcounter_comm = 0\n",
    "\t\t#print('Number of communities that its risk is set:')\n",
    "\t\tfor communiuty_obj in list_selected_communities:\n",
    "\t\t\t#print(counter_comm) \n",
    "\t\t\tcommuniuty_obj.set_infection_rate(sigma,mean_k,cap_for_searching_exact_sol)\n",
    "\t\t\t#print(counter_comm)  \n",
    "\t\t\t#counter_comm += 1 \n",
    "\t\t\t#print(communiuty_obj.name,communiuty_obj.infection_rate)            \n",
    "\t\t#return 0\n",
    "\t\t############################################################\n",
    "\t\t############### setting previous days risk for each comm ###### \n",
    "\t\t#day_pred_map = 83\n",
    "\t\tfor communiuty_obj in list_selected_communities:\n",
    "\t\t\t#print(len(communiuty_obj.risk))\n",
    "\t\t\tfor ind_day in range(len(communiuty_obj.risk)):\n",
    "\t\t\t\tval = (10000)*(communiuty_obj.infection_rate[ind_day]*communiuty_obj.confirmed_daily[0,ind_day])/(communiuty_obj.pop-communiuty_obj.confirmed_daily[0,ind_day])\n",
    "\t\t\t\t#print(val)\n",
    "\t\t\t\tcommuniuty_obj.set_risk(ind_day,max(0,val))\n",
    "\t\t\t#print(counter_comm)  \n",
    "\t\t\tcounter_comm += 1 \n",
    "\t\t\t#print(\"=====\")\n",
    "# \t\t\tif communiuty_obj.name=='eastlosangeles':\n",
    "# \t\t\t\tplt.plot(range(len(communiuty_obj.risk)),communiuty_obj.risk)\n",
    "# \t\t\t\tplt.show()\n",
    "\t\t\t#print(\"comm\",communiuty_obj,'\\n',communiuty_obj.risk)\n",
    "\t\t\t#pred = pred_R_LA(communiuty_obj.actual_name,communiuty_obj.risk[np.newaxis,:83] ,82,7,1)            \n",
    "\t\t\t##communiuty_obj.set_risk(ind_day)\n",
    "\t\t\t#print(\"af\",pred)\n",
    "\t\t\t#communiuty_obj.set_risk(ind_day,max(0,pred))\n",
    "\t\t#return 0\n",
    "\t\t\t#print(communiuty_obj.name,communiuty_obj.infection_rate)             \n",
    "\t\t############################################################\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "##################for ploting histograms ###########  \n",
    "# # \t\tclear_output(wait=True)\n",
    "# \t\thistogram_risk_score(list_communities)\n",
    "# \t\thistogram_R_val(list_communities)\n",
    "# \t\treturn 0\n",
    "####################################################    \n",
    "# \t\tclear_output(wait=True)\n",
    "# # \t\t##hist_thresholds = histogram_risk_score(list_communities)\n",
    "\t\thist_thresholds = [-1.0,.1,1,2]\n",
    "\t\tupdate_csv_file(dict_comm_for_update_csv,hist_thresholds)\n",
    "# \t\tupdate_csv_file_PREDICTION(dict_comm_for_update_csv,hist_thresholds)\n",
    "# \t\tclear_output(wait=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\tnum_day_for_moving_ave = 14\n",
    "\ttop_k_community_with_highest_confirmed = 3 \n",
    "\t# Display mode: daily or cumulative\n",
    "\tdisplay_mode = 'cumulative'\n",
    "\tnumber_of_days_passed_from_16th = 107 - 16 + 1    # downloaded data on June 9,2020\n",
    "\tnumber_of_days_passed_from_16th_used_for_prediction =39\n",
    "\tfuture_day_to_be_predicted = 1\n",
    "\tcriteria = 'train'\n",
    "\t# SEIR model \n",
    "\t# 1-exp(-1/d_I) where d_I is 3-7\n",
    "\tsigma = 1.0/7.5 # 5.2\n",
    "\tgamma = 1.0/(2.3) \n",
    "\t\"\"\" all_lag_indices represent the lags in the model, it should be a list of increamental numbers (min number is 1), \n",
    "\ti.g. [1,3] means using times slots information of t-1 and t-3\"\"\"\n",
    "\t#all_lag_indices = [1]\n",
    "\tmain(num_day_for_moving_ave,top_k_community_with_highest_confirmed,display_mode, 16 + number_of_days_passed_from_16th,future_day_to_be_predicted,number_of_days_passed_from_16th_used_for_prediction,criteria,sigma,gamma)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
